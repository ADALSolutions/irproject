{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We prepare Dataset that will be used in different Machine Learning Models\n",
    "We will use Logistic Regression and Support Vector Machine to obtain weights to use in Condorcet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\RankFusion\"     \n",
    "dir_in = \"input\"   \n",
    "dir_w = \"weights\"       \n",
    "dir_norm = \"norm\"\n",
    "dir_comb = \"comb\"\n",
    "\n",
    "filename_list = listFiles(path, dir_in)\n",
    "\n",
    "Dataset = {}   \n",
    "for filename_in in filename_list:\n",
    "    path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "    in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "    in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "    for i in range(0, in_file.shape[0], 1):\n",
    "        topicID = in_file['topicID'][i]\n",
    "        documentID = in_file['docID'][i]\n",
    "        score = in_file['score'][i]\n",
    "        rank=in_file['rank'][i]\n",
    "        #si potrebbe anche usare lo score, ma non avrebbe molto senso\n",
    "        Dataset.setdefault((topicID, documentID), {})\n",
    "        Dataset[(topicID, documentID)][filename_in]=rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pool=\"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\terrier-core-4.2\\\\share\\\\TIPSTER\\\\pool\\\\qrels.trec7.txt\"\n",
    "in_file = pd.read_csv(path_pool, delimiter = \" \", header = None)\n",
    "in_file.columns = [\"topicID\", \"q0\", \"docID\", \"state\"]\n",
    "\n",
    "DataY={}\n",
    "for i in range(0, in_file.shape[0], 1):\n",
    "    topicID = in_file['topicID'][i]\n",
    "    documentID = in_file['docID'][i]\n",
    "    state = in_file['state'][i]\n",
    "    DataY[(topicID, documentID)]=state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 10276]\n",
      " [    1   943]]\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for d in Dataset:\n",
    "    if len(Dataset[d])==10 and (d in DataY) :\n",
    "        x=[]\n",
    "        for f in filename_list:\n",
    "            x.append(Dataset[d][f])\n",
    "        #print x\n",
    "        X.append(x)\n",
    "        Y.append( DataY[d])\n",
    "\n",
    "X=np.asarray(X)\n",
    "Y=np.asarray(Y)     \n",
    "frequencies=sp.stats.itemfreq(Y) \n",
    "print(np.asarray(frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split Dataset in Train e Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here We try to reduce impact of main weights using logarithm\n",
    "We print values that will be used in the CondorcetLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.084740\n",
      "Best logistic regression test error: 0.082353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg = linear_model.LogisticRegression(solver='saga',n_jobs=-1)\n",
    "logreg=logreg.fit(X_train,y_train)\n",
    "\n",
    "training_error = 1. - logreg.score(X_train,y_train)\n",
    "test_error = 1. - logreg.score(X_test,y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BB2c1.0_1.res', 'BM25b0.75_0.res', 'DFR_BM25c1.0_2.res', 'DLH_3.res', 'DPH_4.res', 'In_expB2c1.0_5.res', 'LemurTF_IDF_6.res', 'LGDc1.0_7.res', 'PL2c1.0_8.res', 'TF_IDF_9.res']\n",
      "[[ 0.34881807  0.63175059  0.76863758  0.46072256 -0.04495694 -0.52890744\n",
      "  -0.75808169 -0.94542233 -0.08658134 -0.52578205]]\n"
     ]
    }
   ],
   "source": [
    "print filename_list\n",
    "print logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use Support Vector Machine on Dataset to train it our model and obtain weights that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.084621\n",
      "Best logistic regression test error: 0.082353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "training_error = 1. - clf.score(X_train,y_train)\n",
    "test_error = 1. - clf.score(X_test,y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We take weights from SVM Linear Model and we transform them in the final weights that will be used in the CondorcetML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_BB2c1.0_1.res', 'n_BM25b0.75_0.res', 'n_DFR_BM25c1.0_2.res', 'n_DLH_3.res', 'n_DPH_4.res', 'n_In_expB2c1.0_5.res', 'n_LemurTF_IDF_6.res', 'n_LGDc1.0_7.res', 'n_PL2c1.0_8.res', 'n_TF_IDF_9.res']\n",
      "[[109.80170621]\n",
      " [131.37245762]\n",
      " [127.43305118]\n",
      " [ 91.39328224]\n",
      " [ 60.49551915]\n",
      " [  5.12221583]\n",
      " [ 11.63204066]\n",
      " [  5.67932307]\n",
      " [ 56.91590495]\n",
      " [  0.15449908]]\n"
     ]
    }
   ],
   "source": [
    "print filename_list\n",
    "#print(clf.coef_)\n",
    "v=(clf.coef_*100)[0]\n",
    "v.shape=(10,1)\n",
    "#print v\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(v)\n",
    "scala = scaler.transform(v)\n",
    "scala=(scala+1.2)*0.5*100\n",
    "print scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here We try to reduce impact of main weights using logarithm\n",
    "We print values that will be used in the CondorcetLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.77875666]\n",
      " [ 7.03751903]\n",
      " [ 6.99359569]\n",
      " [ 6.51401622]\n",
      " [ 5.91875638]\n",
      " [ 2.35676804]\n",
      " [ 3.54003231]\n",
      " [ 2.50571898]\n",
      " [ 5.83075996]\n",
      " [-2.69432984]]\n"
     ]
    }
   ],
   "source": [
    "print(np.log2(scala))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
