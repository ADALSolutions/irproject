{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "path_to_pool = \"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\terrier-core-4.2\\\\share\\\\TIPSTER\\\\pool\\\\qrels.trec7.txt\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Lee95]: Combining Multiple Evidence from Different Properties of Weighting Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_score (path, filename_in, dir_in, dir_out):\n",
    "    path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "    df = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "    df.columns = [\"topicID\", \"q0\", \"docID\" , \"rank\", \"score\" , \"model\"]\n",
    "\n",
    "    summary = stats.describe(df[:]['score'])\n",
    "    minimum = summary[1][0]\n",
    "    maximum = summary[1][1]\n",
    "    norm = np.asarray(df[:]['score'])\n",
    "    norm = (norm - minimum) / float((maximum - minimum))#qui viene fatta una copia della colonna\n",
    "    df[:]['score'] = norm\n",
    "\n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\n_\" + filename_in\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")\n",
    "    \n",
    "def normalize_score_all (filename_list, path, dir_in, dir_out):\n",
    "    for filename in filename_list:\n",
    "        normalize_score(path, filename, dir_in, dir_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [FoxShaw93]: Combination of Multiple Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb_sum (filename_list, path, dir_in, dir_out):\n",
    "    comb_sum = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_sum.setdefault((topicID, documentID), 0)\n",
    "                comb_sum[(topicID, documentID)] += score\n",
    "\n",
    "    comb_sum = sorted(comb_sum.items(), key = lambda (k, v) : (v, k), reverse = True)\n",
    "    comb_sum = np.asarray([list(k) + [v] for k, v in comb_sum])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_sum.txt\"\n",
    "    df = pd.DataFrame(data = comb_sum, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombSum\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb_max (filename_list, path, dir_in, dir_out):\n",
    "    comb_max = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_max.setdefault((topicID, documentID), 0)\n",
    "                if(comb_max[(topicID, documentID)] < score):\n",
    "                      comb_max[(topicID, documentID)] = score\n",
    "    \n",
    "    comb_max = sorted(comb_max.items(), key = lambda (k, v) : (v, k), reverse = True)\n",
    "    comb_max = np.asarray([list(k) + [v] for k, v in comb_max])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_max.txt\"\n",
    "    df = pd.DataFrame(data = comb_max, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombMax\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb_min (filename_list, path, dir_in, dir_out):\n",
    "    comb_min = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_min.setdefault((topicID, documentID), 2) # Ipothesis: normalized values\n",
    "                if(comb_min[(topicID, documentID)] > score):\n",
    "                      comb_min[(topicID, documentID)] = score  \n",
    "    \n",
    "    comb_min = sorted(comb_min.items(), key = lambda (k, v) : (v, k), reverse = True)\n",
    "    comb_min = np.asarray([list(k) + [v] for k, v in comb_min])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_min.txt\"\n",
    "    df = pd.DataFrame(data = comb_min, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombMin\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb_median (filename_list, path, dir_in, dir_out):\n",
    "    comb_median = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_median.setdefault((topicID, documentID), [])\n",
    "                comb_median[(topicID, documentID)].append(score)\n",
    "    for k in comb_median:\n",
    "        median = np.median(np.asarray(comb_median[k]))\n",
    "        comb_median[k] = median\n",
    "\n",
    "    comb_median = sorted(comb_median.items(), key = lambda (k, v) : (v, k), reverse = True)\n",
    "    comb_median = np.asarray([list(k) + [v] for k, v in comb_median])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_median.txt\"\n",
    "    df = pd.DataFrame(data = comb_median, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombMedian\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def comb_mnz (filename_list, path, dir_in, dir_out):\n",
    "    comb_mnz = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_mnz.setdefault((topicID, documentID), [0,0])\n",
    "                comb_mnz[(topicID, documentID)][0] += score\n",
    "                comb_mnz[(topicID, documentID)][1] += 1   \n",
    "    for k in comb_mnz:\n",
    "        comb_mnz[k] = comb_mnz[k][0] * comb_mnz[k][1]\n",
    "        \n",
    "    comb_mnz = sorted(comb_mnz.items(), key = lambda (k ,v) : (v, k), reverse = True)\n",
    "    comb_mnz = np.asarray([list(k[0]) + [k[1]] for k in comb_mnz])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_mnz.txt\"\n",
    "    df = pd.DataFrame(data = comb_mnz, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombMnz\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb_anz (filename_list, path, dir_in, dir_out):\n",
    "    comb_anz = {}   \n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                comb_anz.setdefault((topicID, documentID), [0,0])\n",
    "                comb_anz[(topicID, documentID)][0] += score\n",
    "                comb_anz[(topicID, documentID)][1] += 1\n",
    "    for k in comb_anz:\n",
    "        comb_anz[k] = comb_anz[k][0] / (float)(comb_anz[k][1])\n",
    "\n",
    "    comb_anz = sorted(comb_anz.items(), key = lambda (k, v) : (v, k), reverse = True)\n",
    "    comb_anz = np.asarray([ list(k[0]) + [k[1]] for k in comb_anz])\n",
    "    \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"comb_anz.txt\"\n",
    "    df = pd.DataFrame(data = comb_anz, columns = ['topicID', 'docID', 'score'])\n",
    "    df = df.sort_values(['topicID', 'score'], ascending=[True, False])\n",
    "    df['Rank'] = df.groupby('topicID')['score'].rank(ascending = False).astype('int64') - 1\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"CombAnz\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'Rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CF02]: Condorcet Fusion for Improved Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algorithm1(k1, k2):\n",
    "    count = 0\n",
    "    for filename in filename_list_global:   \n",
    "        flag1 = filename in condorcet[k1]\n",
    "        flag2 = filename in condorcet[k2]\n",
    "        if (flag1 == True and flag2 == False):\n",
    "            count = count + 1\n",
    "        if (flag1 == False and flag2 == True):\n",
    "            count = count - 1\n",
    "        if (flag1 == True and flag2 == True):\n",
    "            if condorcet[k1][filename] > condorcet[k2][filename]:\n",
    "                count += 1\n",
    "            else:\n",
    "                count -= 1\n",
    "    if(count > 0):\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def condorcet_alg (filename_list, path, dir_in, dir_out):\n",
    "    global condorcet\n",
    "    global filename_list_global\n",
    "    condorcet = {}  \n",
    "    L = set({})\n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        \n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                condorcet.setdefault((topicID, documentID), {})\n",
    "                condorcet[(topicID, documentID)][filename_in] = score\n",
    "                L.add((topicID,documentID))        \n",
    "    \n",
    "    filename_list_global = filename_list\n",
    "    L = list(L) \n",
    "    LL = {}\n",
    "    for i in L:\n",
    "        LL.setdefault(i[0], [])\n",
    "        LL[i[0]].append((i[0], i[1]))\n",
    "        \n",
    "    for k in LL:\n",
    "        LL[k] = sorted(LL[k], cmp = algorithm1, reverse = True)\n",
    "        LL[k] = [ np.asarray(list(LL[k][i]) + [i]) for i in range(len(LL[k]))]\n",
    "        \n",
    "        LL[k] = np.asarray(LL[k])\n",
    "       \n",
    "    Matrix=[]\n",
    "    for k in LL:\n",
    "        for i in LL[k]:\n",
    "            Matrix.append(i)\n",
    "    Matrix=np.asarray(Matrix) \n",
    "        \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + \"condorcet.txt\"\n",
    "    df = pd.DataFrame(data = Matrix, columns = ['topicID', 'docID', 'rank'])\n",
    "    df['rank'] = df['rank'].astype('int64')\n",
    "    df['score'] = df['rank'].max() - df['rank']  #1. / (df['rank'].astype('int64') + 1)\n",
    "    df = df.sort_values(['topicID', 'rank'], ascending=[True, True])\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = \"Condorcet\"\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algorithm1_weighted(k1, k2):\n",
    "    count = 0\n",
    "    for filename in filename_list_global:   \n",
    "        flag1 = filename in condorcet[k1]\n",
    "        flag2 = filename in condorcet[k2]\n",
    "        if (flag1 == True and flag2 == False):\n",
    "            count = count + w[filename]\n",
    "        if (flag1 == False and flag2 == True):\n",
    "            count = count -  w[filename]\n",
    "        if (flag1 == True and flag2 == True):\n",
    "            if condorcet[k1][filename] > condorcet[k2][filename]:\n",
    "                count +=  w[filename]\n",
    "            else:\n",
    "                count -=  w[filename]\n",
    "    if(count > 0):\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def condorcet_weighted (filename_list, path, dir_in, dir_out, dir_w,nome_file=\"condorcetWeighted.txt\"):\n",
    "    global w\n",
    "    w = findWeights(path, dir_in, dir_w)\n",
    "    global condorcet\n",
    "    global filename_list_global\n",
    "    condorcet = {}  \n",
    "    L = set({})\n",
    "    for filename_in in filename_list:\n",
    "        path_in = path + \"\\\\\" + dir_in + \"\\\\\" + filename_in\n",
    "        in_file = pd.read_csv(path_in, delimiter = \" \", header = None)\n",
    "        in_file.columns = [\"topicID\", \"q0\", \"docID\", \"rank\", \"score\", \"model\"]\n",
    "        \n",
    "        for i in range(0, in_file.shape[0], 1):\n",
    "                topicID = in_file['topicID'][i]\n",
    "                documentID = in_file['docID'][i]\n",
    "                score = in_file['score'][i]\n",
    "                condorcet.setdefault((topicID, documentID), {})\n",
    "                condorcet[(topicID, documentID)][filename_in] = score\n",
    "                L.add((topicID,documentID))        \n",
    "    \n",
    "    filename_list_global = filename_list\n",
    "    L = list(L) \n",
    "    LL = {}\n",
    "    for i in L:\n",
    "        LL.setdefault(i[0], [])\n",
    "        LL[i[0]].append((i[0], i[1]))\n",
    "        \n",
    "    for k in LL:\n",
    "        LL[k] = sorted(LL[k], cmp = algorithm1_weighted, reverse = True)\n",
    "        LL[k] = [ np.asarray(list(LL[k][i]) + [i]) for i in range(len(LL[k]))]\n",
    "        \n",
    "        LL[k] = np.asarray(LL[k])\n",
    "       \n",
    "    Matrix=[]\n",
    "    for k in LL:\n",
    "        for i in LL[k]:\n",
    "            Matrix.append(i)\n",
    "    Matrix=np.asarray(Matrix) \n",
    "        \n",
    "    path_out = path + \"\\\\\" + dir_out + \"\\\\\" + nome_file\n",
    "    df = pd.DataFrame(data = Matrix, columns = ['topicID', 'docID', 'rank'])\n",
    "    df['rank'] = df['rank'].astype('int64')\n",
    "    df['score'] = df['rank'].max() - df['rank']  #1. / (df['rank'].astype('int64') + 1)\n",
    "    df = df.sort_values(['topicID', 'rank'], ascending=[True, True])\n",
    "    df['Q0'] = \"Q0\"\n",
    "    df['model'] = nome_file\n",
    "    df = df.reindex(columns = ['topicID', 'Q0', 'docID', 'rank', 'score', 'model'])\n",
    "    df.to_csv(path_out, index = False, header = False, sep = \" \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listFiles(path, directory):\n",
    "    filename_list = []\n",
    "    for file in os.listdir(path + \"\\\\\" + directory):\n",
    "        if(os.path.isfile(os.path.join(path + \"\\\\\" + directory, file))):\n",
    "            filename_list.append(file)\n",
    "    return filename_list\n",
    "\n",
    "def manageDirectory(path, directory):\n",
    "    if os.path.exists(path + \"\\\\\" + directory):\n",
    "        os.chmod(path + \"\\\\\" + directory, 0777)\n",
    "        #os.remove(path + \"\\\\\" + directory)\n",
    "        shutil.rmtree(path + \"\\\\\" + directory)\n",
    "    os.mkdir(path + \"\\\\\" + directory)    \n",
    "    \n",
    "def findWeights(path, directory, directory_weights,exist=True):\n",
    "    filename_list = listFiles(path, directory)\n",
    "    weights={}\n",
    "    for filename in filename_list:\n",
    "        map_value=take_MAP(path,directory,directory_weights,filename,exist)\n",
    "        weights[filename]=map_value\n",
    "    return weights\n",
    "\n",
    "def take_MAP_TrecEval(path,directory, directory_weights,filename,exist=False):\n",
    "    if(not exist):\n",
    "        process = \"{}trec_eval {} {}\"\n",
    "        path_to_bin = \"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\terrier-core-4.2\\\\bin\\\\\"\n",
    "        path_to_pool = \"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\terrier-core-4.2\\\\share\\\\TIPSTER\\\\pool\\\\qrels.trec7.txt\"\n",
    "        path_to_run = path + \"\\\\\" + directory + \"\\\\\" + str(filename)\n",
    "        process=str(process.format(path_to_bin, path_to_pool, path_to_run))\n",
    "        #print(\"take MAP \\n \"+process)\n",
    "        p = subprocess.check_output(process, shell=True)\n",
    "\n",
    "        text_file = open(path+\"\\\\\" + directory_weights + \"\\\\\" + \"w_\"+ filename, \"w\")\n",
    "        text_file.write(p)\n",
    "        text_file.close()\n",
    "        \n",
    "    text_file = open(path+\"\\\\\" + directory_weights + \"\\\\\" + \"w_\"+ filename, \"r\")\n",
    "    p=text_file.read()\n",
    "     \n",
    "    #print p\n",
    "    lines=p.split(\"\\n\")        \n",
    "    return float(lines[6].split()[2])\n",
    "\n",
    "def take_MAP(path,directory, directory_weights,filename,exist=False):\n",
    "    if(not exist):\n",
    "        eng.addpath(\"r\"+path, nargout=0)\n",
    "        path_to_run = path + \"\\\\\" + directory + \"\\\\\" + str(filename)\n",
    "        name_run=filename\n",
    "        eng.workspace['path_to_pool']=path_to_pool\n",
    "        eng.workspace['path_to_run']=path_to_run\n",
    "        eng.workspace['name_run']=name_run\n",
    "        eng.RankFusion(nargout=0)\n",
    "        somma=eng.workspace['sum']\n",
    "\n",
    "        text_file = open(path+\"\\\\\" + directory_weights + \"\\\\\" + \"w_\"+ filename, \"w\")\n",
    "        text_file.write(str(somma))\n",
    "        text_file.close()\n",
    "        \n",
    "    text_file = open(path+\"\\\\\" + directory_weights + \"\\\\\" + \"w_\"+ filename, \"r\")\n",
    "    p=text_file.read()\n",
    "       \n",
    "    return float(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Could use Empty Line below if dirs are in the same python's working dir\n",
    "    path = \"C:\\\\Users\\\\DavideDP\\\\AnacondaProjects\\\\Project\\\\RankFusion\" \n",
    "    \n",
    "    dir_in = \"input\"   \n",
    "    #filename_list = listFiles(path, dir_in)\n",
    "    \n",
    "    dir_w = \"weights\"   \n",
    "    \n",
    "    dir_norm = \"norm\"\n",
    "    #If we already have normalized files than we can comment the next two rows\n",
    "    #manageDirectory(path, dir_norm)\n",
    "    #normalize_score_all(filename_list, path, dir_in, dir_norm)\n",
    "    \n",
    "    filename_list = listFiles(path, dir_norm)\n",
    "    #If we want execute only some of the algorithm\n",
    "    #we can comment line of manageDirectory and all the call to function of algorthims that we don't want\n",
    "    dir_comb = \"comb\"\n",
    "    manageDirectory(path, dir_comb)\n",
    "    comb_sum(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombSum terminated without errors\"\n",
    "    comb_max(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombMax terminated without errors\"\n",
    "    comb_min(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombMin terminated without errors\"\n",
    "    comb_median(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombMedian terminated without errors\"\n",
    "    comb_mnz(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombMnz terminated without errors\"\n",
    "    comb_anz(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"CombAnz terminated without errors\"\n",
    "    condorcet_alg(filename_list, path, dir_norm, dir_comb)\n",
    "    print \"Condorcet terminated without errors\"\n",
    "    condorcet_weighted(filename_list, path, dir_norm, dir_comb, dir_w)\n",
    "    print \"Condorcet Weighted terminated without errors\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
